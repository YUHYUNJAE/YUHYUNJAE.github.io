<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>딥러닝 1단계 : 신경망과 딥러닝(2) | HYUNJAE&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="2. 신경망과 로지스틱회귀1강 - 이진분류 학습목표 : 로지스틱 분류(Logistic Classification)을 이야기 하기 전에 이진분류의 개념을 이해하고, 문제 설정과 표기법을 정의할 수 있음  keyword : 이진분류(Binary Classification), 로지스틱 분류(Logistic Classification)  학습내용 :  1) 신경망">
<meta name="keywords" content="Deep Learning step.1">
<meta property="og:type" content="article">
<meta property="og:title" content="딥러닝 1단계 : 신경망과 딥러닝(2)">
<meta property="og:url" content="http:&#x2F;&#x2F;yuhyunjae.github.io&#x2F;2019&#x2F;11&#x2F;23&#x2F;Deeplearning_step1-(2)&#x2F;index.html">
<meta property="og:site_name" content="HYUNJAE&#39;s Blog">
<meta property="og:description" content="2. 신경망과 로지스틱회귀1강 - 이진분류 학습목표 : 로지스틱 분류(Logistic Classification)을 이야기 하기 전에 이진분류의 개념을 이해하고, 문제 설정과 표기법을 정의할 수 있음  keyword : 이진분류(Binary Classification), 로지스틱 분류(Logistic Classification)  학습내용 :  1) 신경망">
<meta property="og:locale" content="kor">
<meta property="og:image" content="https:&#x2F;&#x2F;cphinf.pstatic.net&#x2F;mooc&#x2F;20181026_62&#x2F;154051753591861ru0_PNG&#x2F;derivatives.PNG">
<meta property="og:image" content="https:&#x2F;&#x2F;cphinf.pstatic.net&#x2F;mooc&#x2F;20180615_270&#x2F;1529026981725rnVc0_PNG&#x2F;image.PNG">
<meta property="og:updated_time" content="2019-11-22T15:00:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https:&#x2F;&#x2F;cphinf.pstatic.net&#x2F;mooc&#x2F;20181026_62&#x2F;154051753591861ru0_PNG&#x2F;derivatives.PNG">
  
    <link rel="alternate" href="/atom.xml" title="HYUNJAE&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">HYUNJAE&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yuhyunjae.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Deeplearning_step1-(2)" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/11/23/Deeplearning_step1-(2)/" class="article-date">
  <time datetime="2019-11-22T15:00:00.000Z" itemprop="datePublished">2019-11-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      딥러닝 1단계 : 신경망과 딥러닝(2)
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="2-신경망과-로지스틱회귀"><a href="#2-신경망과-로지스틱회귀" class="headerlink" title="2. 신경망과 로지스틱회귀"></a>2. 신경망과 로지스틱회귀</h1><h2 id="1강-이진분류"><a href="#1강-이진분류" class="headerlink" title="1강 - 이진분류"></a>1강 - 이진분류</h2><ul>
<li><p>학습목표 : 로지스틱 분류(Logistic Classification)을 이야기 하기 전에 이진분류의 개념을 이해하고, 문제 설정과 표기법을 정의할 수 있음</p>
</li>
<li><p>keyword : 이진분류(Binary Classification), 로지스틱 분류(Logistic Classification)</p>
</li>
<li><p>학습내용 :<br>  1) 신경망에서 학습하는 방법은 정방향 전파와 역전파가 있음<br>  2) 이진 분류란 그렇다/아니다 2개로 분류하는 것. 이때 결과가 ‘그렇다’이면 1로, ‘아니다’이면 0으로 표현<br>  3) 로지스틱 회귀란 이진분류를 하기 위해 사용되는 알고리즘</p>
</li>
<li><p>강의 내용 :</p>
<ul>
<li>로지스틱 회귀는 이진분류를 위해 필요한 알고리즘<ul>
<li>이진 분류의 목표는 입력된 사진을 나타내는 특성 벡터 x를 가지고 y가 1 아니면 0을 예측할 수 있는 분류기를 학습하는 것</li>
<li>표기법</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>$$ X = \begin{bmatrix}<br>\vdots &amp; \vdots &amp; &amp; \vdots \<br>x^{(1)} &amp; x^{(2)} &amp; \dots  &amp; x^{(x)} \<br>\vdots &amp; \vdots &amp; &amp; \vdots<br>\end{bmatrix}$$</p>
<p>  row = $n_{x}$, col = $m$</p>
<p>$$Y = \begin{bmatrix}<br>y^{(1)}&amp; y^{(2)}&amp; \dots&amp; y^{(m)}<br>\end{bmatrix}$$</p>
<p>  -python : X.shape( $n_x,m$), Y.shape($1, m$)</p>
<h2 id="2강-로지스틱-회귀"><a href="#2강-로지스틱-회귀" class="headerlink" title="2강 - 로지스틱 회귀"></a>2강 - 로지스틱 회귀</h2><ul>
<li><p>학습목표 : 로지스틱 회귀 모델이 무엇인지 알 수 있음</p>
</li>
<li><p>keyword : 로지스틱 회귀(Logistic Regression), 예측값 ($\hat y$), 시그모이드(Sigmoid) 함수</p>
</li>
<li><p>학습내용 :<br>  1) 로지스틱 회귀란 답이 0 또는 1로 정해져있는 이진 분류 문제에 사용되는 알고리즘<br>  2) X(입력 특성), y(주어진 입력특성 X에 해당하는 실제 값), $\hat y$(y의 예측값)을 의미<br>  3) 더 자세히 이진 분류를 위한 $\hat y$값은 y가 1일 확률을 의미, $0 \leq \hat y \leq 1$ 사이의 값을 가짐<br>  4) 선형 회귀시 $\hat y = \sigma(W^TX + b)$로 구함</p>
</li>
</ul>
<p>참고: 시그모이드 함수 $sigma(z) = {1}\over{1+e^{-z}}$</p>
<pre><code>![Sigmoid 함수](https://cphinf.pstatic.net/mooc/20180605_88/1528177200744k2iiP_PNG/sigmoid.PNG)</code></pre><h2 id="3강-로지스틱-회귀의-비용함수"><a href="#3강-로지스틱-회귀의-비용함수" class="headerlink" title="3강 - 로지스틱 회귀의 비용함수"></a>3강 - 로지스틱 회귀의 비용함수</h2><ul>
<li><p>학습목표 : 로지스틱 회귀의 비용 함수를 구할 수 있음</p>
</li>
<li><p>keyword : 손실 함수(Loss Function), 비용 함수(Cost Function)</p>
</li>
<li><p>학습내용 :</p>
<p>  1) 우리의 목표는 실제값(y)에 가까운 예측값($\hat y$)를 구하는 것</p>
<p>  2) 손실 함수는 하나의 입력특성(x)에 대한 실제값(y)과 에측값($\hat y$)의 오차를 계산하는 함수</p>
<p>  3) 보통 손실함수는 $$L(\hat y, y) = {{1}\over{2}}(\hat y - y)^2$$ 식을 사용하지만 로지스틱 회귀에서 이러한 손실 함수를 사용하면 지역 최소값에 빠질 수 있기 때문에 사용하지 않음</p>
<p>  4) 로지스틱 회귀에서 사용하는 손실 함수<br>$$L(\hat y, y) = -(y log \hat y + (1-y)log(1-\hat y)$$</p>
<p>  5) 이 함수를 직관적으로 이해하기 위해 두 가지 경우로 나누어 생각해 볼 수 있다.</p>
<pre><code>- y = 0 인 경우 $L(\hat y, y)= -log(1-\hat y)$가 0에 가까워지도록 $\hat y$는 0에 수렴
- y = 1 인 경우 $L(\hat y, y)= -log\hat y$가 0에 가까워지도록 $\hat y$는 1에 수렴</code></pre><p>  6) 하나의 입력에 대한 오차를 계산하는 함수를 <code>손실 함수</code></p>
<p>  7) 모든 입력에 대한 오차를 계산하는 함수를 <code>비용 함수</code></p>
<p>  8) 비용 함수는 모든 입력에 대해 계산함 손실 함수의 평균 값으로 구할 수 있으면 식으로 나타내면</p>
<p>  $$J(w,b) = -{1}over{m} \Sigma^{m}_{i=1} (y^{(i)}log\hat y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})$$</p>
<p>  9) 실제값과 비슷한 예측값을 원하며, 비용 함수의 값이 작아지도록 하는 w와 b를 찾는 것이 목표</p>
</li>
</ul>
<h2 id="4강-경사하강법"><a href="#4강-경사하강법" class="headerlink" title="4강 - 경사하강법"></a>4강 - 경사하강법</h2><ul>
<li><p>학습목표 : 경사하강법(Gradient Descent)을 알 수 있음</p>
</li>
<li><p>keyword : 경사하강법(Gradient Descent), 도함수(Derivative), 기울기(Slope), 볼록한(Convex)</p>
</li>
<li><p>학습내용 :<br>  1) 비용함수가 전체 데이터셋의 예측이 얼마나 잘 평가되었는지 보는 것이라면, <code>경사하강법</code>은 이를 가능케하는 파라미터 w와 b를 찾아내는 방법 중 하나<br>  2) 비용 함수는 볼록한 형태여야 하며, 볼록하지 않은 함수를 쓰게 되면, 경사하강법을 통해 최적의 파라미터를 찾을 수 없음<br>  3) 함수의 최소값을 모르기 때문에 임의의 점을 골라서 시작<br>  4) 경사하강법은 가장 가파른(steepest) 방향, 즉 함수의 기울기를 따라서 최적의 값을로 한 스텝씩 업데이트하게 됨.<br>  5) 알고리즘</p>
<ul>
<li><p>$w : w-\alpha{dJ(w,b)}\over{dw}$</p>
</li>
<li><p>$b : b-\alpha{dJ(w,b)}\over{db}$</p>
</li>
<li><p>$\alpha$ : 학습률이라고 하며, 얼만큼의 스텝으로 나아갈 것인지</p>
</li>
<li><p>${dJ(w)} \over {dw}$ : 도함수라고 하며, 미분을 통해 구한 값, dw 라고 표기하기도 함    </p>
</li>
</ul>
<p>6) 만약 dw $\succ$ 0 이면, 파라미터 w는 기존의 w 값 보다 작은 방향으로 업데이트 될 것이고, 만약 dw $\prec$ 0 이면, 파라미터 w는 기본의 w 값 보다 큰 방향으로 업데이트 될 것임.<br>7) 도함수는 함수의 기울기라고 볼 수 있음<br>8) 하나의 변수에 대한 도함수는 $dw ={df(w)}\over{dw}$라고 표기하지만 두 개 이상은 보통 아래와 같이 표현</p>
<ul>
<li>$dw = {dJ(w,b)} \over {dw}$ : 함수의 기울기가 w 방향으로 얼만큼 변했는지 나타냄</li>
<li>$db = {dJ(w,b)} \over {db}$ : 함수의 기울기가 b 방향으로 얼만큼 변했는지 나타냄</li>
</ul>
</li>
</ul>
<h2 id="5강-미분"><a href="#5강-미분" class="headerlink" title="5강 - 미분"></a>5강 - 미분</h2><ul>
<li><p>학습목표 : 1차 함수의 미분을 이해</p>
</li>
<li><p>keyword : 기울기(slope), 도함수(derivative)</p>
</li>
<li><p>학습내용 :<br>  1) 도함수(=어떤 함수의 기울기)란 변수 a를 조금만 변화했을 때, 함수 f(a)가 얼만큼 변하는지 측정하는 것<br>  2) 표기법:<br>  ${d}\over{da} f(a)={d f(a)}\over{da}$</p>
</li>
</ul>
<h2 id="6강-더-많은-미분-예제"><a href="#6강-더-많은-미분-예제" class="headerlink" title="6강 - 더 많은 미분 예제"></a>6강 - 더 많은 미분 예제</h2><ul>
<li><p>학습목표 : 젣제다른 함수의 미부</p>
</li>
<li><p>keyword : 기울기(slope), 도함수(derivative)</p>
</li>
<li><p>학습내용 :<br>  1) 기울기 혹은 도함수는 함수에 따라서 달라 질 수 있음<br>  2) 예제 :<br>  <img src="https://cphinf.pstatic.net/mooc/20181026_62/154051753591861ru0_PNG/derivatives.PNG" alt="함수에 따른 도함수"></p>
</li>
</ul>
<h2 id="7강-계산-그래프"><a href="#7강-계산-그래프" class="headerlink" title="7강 - 계산 그래프"></a>7강 - 계산 그래프</h2><ul>
<li><p>학습목표 : 계산 그래프가 무엇인지 앎</p>
</li>
<li><p>keyword : 계산 그래프(Computation Graph)</p>
</li>
<li><p>학습내용 :</p>
<ul>
<li>$J(a,b,c) = 3(a+bc)$의 계산 그래프 만드는 과정<br> (1) u = bc<br> (2) v = a+u<br> (3) J = 3v</li>
<li>도식화<br><img src="https://cphinf.pstatic.net/mooc/20180615_270/1529026981725rnVc0_PNG/image.PNG" alt="J(a,b,c) = 3(a+bc) 도식화"></li>
</ul>
</li>
</ul>
<h2 id="8강-계산-그래프로-미분하기"><a href="#8강-계산-그래프로-미분하기" class="headerlink" title="8강 - 계산 그래프로 미분하기"></a>8강 - 계산 그래프로 미분하기</h2><ul>
<li><p>학습목표 : 계산 그래프를 통해 미분과정을 이해할 수 있다.</p>
</li>
<li><p>keyword : 계산 그래프(Computation Graph), 미분의 연쇄법칙(Chain Rule)</p>
</li>
<li><p>학습내용 :<br>  1) 미분의 연쇄법칙이란 합성함수의 도함수에 대한 공식임. 합성함수의 도함수는 합성함수를 구성하는 함수의 미분을 곱함으로써 구할 수 있음</p>
<pre><code>- 입력변수 a를 통해서 J까지 도달 하기 위해서 $a→v→J$ 의 프로세스로 진행됨. 즉, 변수의 a만 보게된다면, $J = J(u(a))$라는 합성함수가 될것 임.
- 강의에서 합성함수를 a로 미분한 값 ${dJ}\over{da}$를 구하기 위해서 ${dJ}\over{dv}$와</code></pre><p>${dv}\over{da}$의 곱으로 표현 함. 이러한 법칙이 연쇄법칙 이다.<br>  2) 코드 작성시 편의를 위해서 표기법을 아래와 같이 정의</p>
<pre><code>- 최종변수를 Final Output var, 미분하려고 하는 변수를 var라고 정의
- ${d Final output var}\over{d var}=d var$</code></pre></li>
</ul>
<h2 id="9강-로지스틱-회귀의-경사하강법"><a href="#9강-로지스틱-회귀의-경사하강법" class="headerlink" title="9강 - 로지스틱 회귀의 경사하강법"></a>9강 - 로지스틱 회귀의 경사하강법</h2><ul>
<li><p>학습목표 : 계산 그래프를 통해 로지스틱 회귀 미분과정을 이해할 수 있음</p>
</li>
<li><p>keyword : 계산 그래프(Computation Graph), 로지스틱 회귀(Logistic Regression), 경사 하강법(Gradient Descent)</p>
</li>
<li><p>학습내용 :<br>  1) $z= w_{1}x_{1}+w_{2}x_{2}+b → \hat{y} = a = \sigma(z) → L(a,y)$</p>
<pre><code>- 로지스특 회귀분석의 목적은 w와 b를 조절하여 손실함수(L)을 줄이는 것.</code></pre><p>  2) $da = {dL(a,y)} \over {da} = -{y} \over {a} + {1-y}\over{1-a}$<br>  3) $dz = {dL} \over {dz} = {dL(a,y)} \over {dz} = {dL} \over {da} {da}\over{dz} = a - y$<br>  4) $dw_{1} = {dL}\over{dw_{1}} = x_{1}dz$<br>  5) $db = {dL}\over{db} = dz$</p>
</li>
</ul>
<h2 id="10강-m개-샘플의-경사하강법"><a href="#10강-m개-샘플의-경사하강법" class="headerlink" title="10강 - m개 샘플의 경사하강법"></a>10강 - m개 샘플의 경사하강법</h2><ul>
<li><p>학습목표 : 경사 하강법을 예제에 적용할 수 있음</p>
</li>
<li><p>keyword : 경사 하강법(Gradient Descent)</p>
</li>
<li><p>학습내용 :<br>  1) $J(w,b) = {{1}\over{m}}\Sigma^{m}_{i=1}L(a^{(i)},y^{(i)})$</p>
<pre><code>$a^{(i)} = \hat y^{(i)} = \sigma(z^{(i)}) = \sigma(w^{T}x^{(i)}+b)$</code></pre><p>  2) ${d}\over{dw_{1}} J(w,b) = {1}\over{m} \Sigma^{m}<em>{i=1} {d} \over {dw</em>{1}} L(a^{(i)},y^{(i)})$</p>
<pre><code>${d}\over{dw_{1}} L(a^{(i)},y^{(i)}) = dw_{1}^{(i)}-(x^{(i)},y^{(i)})$</code></pre><p>  3) 위의 수식을 코드로 표현</p>
<pre><code>![m개 샘플의 경사하강법 코드](https://cphinf.pstatic.net/mooc/20180615_249/1529027949584hxeeh_PNG/image.PNG)

- 위의 코드에서는 특성의 개수를 2개로 가정, 만약 특성의 개수가 많아진다면 이 또한 for문을 이용해 처리해야 함. 즉, 이중 for문을 사용하게 되며 이로인해 계산속도가 느려지게 됨.</code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yuhyunjae.github.io/2019/11/23/Deeplearning_step1-(2)/" data-id="ck3blcpyz00016suj0qep9nt4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning-step-1/" rel="tag">Deep Learning step.1</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2019/11/19/Deeplearning_step1-(1)/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">딥러닝 1단계 : 신경망과 딥러닝(1)</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Deep-Learning/">Deep Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deep-Learning-step-1/" rel="tag">Deep Learning step.1</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Deep-Learning-step-1/" style="font-size: 10px;">Deep Learning step.1</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/11/23/Deeplearning_step1-(2)/">딥러닝 1단계 : 신경망과 딥러닝(2)</a>
          </li>
        
          <li>
            <a href="/2019/11/19/Deeplearning_step1-(1)/">딥러닝 1단계 : 신경망과 딥러닝(1)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 HyunJae Yu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>